# Token Usage Analysis - Phase 3 Optimizations

## Current Token Usage by Tool (v2.6.0)

### High Token Consumption Tools (Priority 1)

#### 1. `read_file` - **VERY HIGH TOKEN USAGE**
- **Current**: Returns entire file content
- **Problem**: Large files = thousands of tokens
- **Examples**:
  - 1KB file: ~250 tokens
  - 10KB file: ~2,500 tokens
  - 100KB file: ~25,000 tokens
  - 1MB file: ~250,000 tokens (exceeds context!)
- **Impact**: ðŸ”´ CRITICAL - Single biggest token consumer
- **Solution**: `read_summary`, `max_lines` parameter, intelligent truncation

#### 2. `list_directory` - **HIGH TOKEN USAGE**
- **Current (verbose)**: ~800 tokens for 50 items
- **Current (compact)**: ~100 tokens for 50 items
- **Problem**: Still verbose in compact mode for large directories
- **Examples**:
  - 50 items compact: 100 tokens
  - 500 items compact: 800 tokens
  - 1000 items: 1,500 tokens
- **Impact**: ðŸŸ  HIGH - Common operation
- **Solution**: Smarter truncation, pagination hint

#### 3. `smart_search` + `advanced_text_search` - **HIGH TOKEN USAGE**
- **Current (verbose)**: 5,000-8,000 tokens for 100 matches
- **Current (compact)**: 200-500 tokens for 100 matches
- **Problem**: Even compact mode can be large with many matches
- **Examples**:
  - 10 matches: 50 tokens
  - 100 matches: 500 tokens
  - 1000 matches: 3,000 tokens
- **Impact**: ðŸŸ  HIGH - Frequent in large codebases
- **Solution**: Better truncation, relevance scoring

#### 4. `get_file_info` - **MEDIUM TOKEN USAGE**
- **Current**: Returns all metadata
- **Problem**: Unnecessary details in most cases
- **Examples**:
  - Verbose: ~150 tokens
  - Compact: ~30 tokens
- **Impact**: ðŸŸ¡ MEDIUM - Moderately frequent
- **Solution**: Already optimized, maybe more compact

### Tool Descriptions - **MODERATE TOKEN WASTE**

#### Current Issues:
- Long descriptions sent in EVERY request
- Redundant explanations
- Examples embedded in descriptions

**Current total description tokens**: ~2,000 tokens per request

**Examples**:
```
"Write large files efficiently using intelligent chunking" (8 tokens)
vs
"Write large files" (3 tokens) - 62% reduction
```

### Error Messages - **LOW BUT FREQUENT**

#### Current Issues:
- Verbose error formatting
- Stack traces in some cases
- Repetitive context

**Impact**: ðŸŸ¡ MEDIUM - Not huge per error, but accumulates

---

## Proposed Optimizations

### 1. **Read Operations Optimization** ðŸ”´ PRIORITY 1

#### A. New Tool: `read_summary`
```json
{
  "tool": "read_summary",
  "arguments": {
    "path": "file.txt",
    "mode": "head",  // head, tail, both, lines
    "lines": 50
  }
}
```

**Token Savings**: 95-99% for large files

#### B. Add `max_lines` parameter to `read_file`
```json
{
  "tool": "read_file",
  "arguments": {
    "path": "file.txt",
    "max_lines": 100  // optional, defaults to unlimited
  }
}
```

**Token Savings**: 50-90% depending on file size

#### C. Intelligent truncation with continuation hint
```
[File truncated: showing first 100 of 5000 lines. Use max_lines parameter or read_summary for specific sections]
```

### 2. **Response Caching** ðŸŸ  PRIORITY 2

#### Cache frequently requested data:
- `performance_stats` - Cache for 30 seconds
- `get_file_info` - Cache for 60 seconds per file
- `list_directory` - Cache for 30 seconds per directory
- `get_optimization_suggestion` - Cache per file until modified

**Token Savings**: 50-80% for repeated requests

**Implementation**:
```go
type ResponseCache struct {
    cache map[string]CachedResponse
    ttl   time.Duration
}

type CachedResponse struct {
    data      string
    timestamp time.Time
}
```

### 3. **Tool Description Optimization** ðŸŸ  PRIORITY 2

#### Reduce verbosity while maintaining clarity:

**Before (210 tokens for all descriptions)**:
```
"Read file with ultra-fast caching and memory mapping"
"Write file with atomic operations and backup"
"List directory with intelligent caching"
```

**After (85 tokens - 60% reduction)**:
```
"Read file (cached, fast)"
"Write file (atomic)"
"List directory (cached)"
```

### 4. **Smart Truncation System** ðŸŸ¡ PRIORITY 3

#### Implement intelligent truncation:
- Show first N matches + total count
- Relevance scoring for search results
- Adaptive limits based on result size

**Example**:
```
// Instead of 1000 matches (10,000 tokens)
"Found 1000 matches. Top 10 most relevant:
file1.js:42, file2.js:15, ...
Use --max-results to see more"
```

**Token Savings**: 80-95% for large result sets

### 5. **Streaming Incremental Results** ðŸŸ¡ PRIORITY 3

#### For very large operations:
- Return partial results as they're found
- Stop when context limit approached
- Provide continuation token

**Token Savings**: Prevents context overflow

### 6. **Parallel Search Optimization** ðŸŸ¢ PRIORITY 4

#### Performance improvement (not token focused):
- Multi-threaded directory traversal
- Worker pool for file processing
- Faster = fewer timeout retries = fewer repeated tokens

---

## Implementation Priority

### Phase 3.1 - Critical Token Savers (This Session)
1. âœ… `read_summary` tool
2. âœ… `max_lines` parameter for `read_file`
3. âœ… Response caching system
4. âœ… Tool description optimization

**Expected Savings**: 60-80% for common workflows

### Phase 3.2 - Advanced Optimizations (Next Session)
5. Smart truncation system
6. Streaming incremental results
7. Error message optimization
8. Parallel search

**Expected Savings**: Additional 10-20%

---

## Token Savings Projections

### Typical Claude Desktop Session (100 operations):

#### Current State (v2.6.0 with compact-mode):
- Tool descriptions: 32 tools Ã— 50 tokens = 1,600 tokens
- 20 read_file (avg 50 lines): 20 Ã— 1,250 = 25,000 tokens
- 10 list_directory: 10 Ã— 100 = 1,000 tokens
- 10 searches: 10 Ã— 500 = 5,000 tokens
- 30 write/edit: 30 Ã— 20 = 600 tokens
- 30 other: 30 Ã— 50 = 1,500 tokens
**TOTAL: ~34,700 tokens**

#### After Phase 3.1 Optimizations:
- Tool descriptions: 32 tools Ã— 20 tokens = 640 tokens (-60%)
- 20 read_file (with max_lines): 20 Ã— 250 = 5,000 tokens (-80%)
- 10 list_directory (cached 50%): 5 Ã— 100 = 500 tokens (-50%)
- 10 searches (cached 30%): 7 Ã— 500 = 3,500 tokens (-30%)
- 30 write/edit: 30 Ã— 20 = 600 tokens (same)
- 30 other (cached 40%): 18 Ã— 50 = 900 tokens (-40%)
**TOTAL: ~11,140 tokens**

### **Overall Savings: 67.9% reduction (23,560 tokens saved)** ðŸŽ‰

---

## Success Metrics

### Token Efficiency:
- âœ… Average read_file: <500 tokens (currently >2,000)
- âœ… Average session: <15,000 tokens (currently ~35,000)
- âœ… Cache hit rate: >40% for common operations

### Performance:
- âœ… No regression in speed
- âœ… Cache overhead: <5ms
- âœ… Memory usage: <100MB total

### User Experience:
- âœ… No breaking changes
- âœ… Backward compatible
- âœ… Optional optimizations (can be disabled)

---

## Configuration

### New Flags for Phase 3:
```bash
--enable-response-cache      # Enable response caching (default: true)
--cache-ttl=30s              # Cache TTL for responses
--default-max-lines=100      # Default max lines for read operations
--aggressive-truncation      # More aggressive token reduction
--smart-descriptions         # Use shorter tool descriptions
```

### Recommended for Maximum Token Savings:
```bash
mcp-filesystem-ultra.exe \
  --compact-mode \
  --enable-response-cache \
  --cache-ttl=60s \
  --default-max-lines=100 \
  --aggressive-truncation \
  --smart-descriptions \
  --max-search-results=20
```

---

**Version**: 3.0.0 (Planned)
**Focus**: Performance + Token Optimization
**Target**: 70% token reduction vs v2.6.0
